{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation of Bandit Learning Algorithms\n",
    "**Author:**  \n",
    "\n",
    "Li Ge (2022533011，李舸)  \n",
    "\n",
    "Long Yuxuan (2022533034, 龙宇轩)  \n",
    "\n",
    "(In lexicographic order)\n",
    "\n",
    "**Contribution:**\n",
    "\n",
    "李舸：part I主要代码及分析部分, part II task2反例\n",
    "\n",
    "龙宇轩: part I绘图部分及图像结论, part II主要代码及分析部分\n",
    "\n",
    "以下文档内容均为两人讨论后的结果\n",
    "\n",
    "(In lexicographic order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Classical Bandit Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some necessary Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Setting the time slots\n",
    "N = 5000\n",
    "# Setting the number of trials\n",
    "trialNumber = 200\n",
    "# Setting the number of arms\n",
    "armNumber = 3\n",
    "# Setting the Bernoulli distribution parameter for each arm\n",
    "theta = [0.7, 0.5, 0.4]\n",
    "\n",
    "# Sampling from Bernoulli distribution with parameter p\n",
    "def bernSample(p):\n",
    "    u = np.random.uniform(0,1)\n",
    "    if u <= p:\n",
    "        sample = 1\n",
    "    else:\n",
    "        sample = 0\n",
    "    return sample\n",
    "\n",
    "# Sampling from Beta distribution with parameters a and b\n",
    "def betaSample(a, b):\n",
    "    sample = np.random.beta(a, b)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the theoretically maximized expectation of aggregate rewards over $N$ time slots (the oracle value):\n",
    "$$\n",
    "E = N \\cdot \\max{\\{0.7, 0.5, 0.4\\}} = 3500\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the classical bandit algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. $\\epsilon$-greedy Algorithm (0 ≤ $\\epsilon$ ≤ 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy Algorithm (0 ≤ epsilon ≤ 1):\n",
      "\n",
      "Epsilon = 0.1:\n",
      "Mean aggregated rewards: 3178.035000\n",
      "Mean regret: 321.965000\n",
      "\n",
      "Epsilon = 0.5:\n",
      "Mean aggregated rewards: 2912.020000\n",
      "Mean regret: 587.980000\n",
      "\n",
      "Epsilon = 0.9:\n",
      "Mean aggregated rewards: 2650.320000\n",
      "Mean regret: 849.680000\n"
     ]
    }
   ],
   "source": [
    "# Epsilon-Greedy Algorithm (0 ≤ epsilon ≤ 1)\n",
    "# randomly chosen from{1,2,3}\n",
    "def randomChoose():\n",
    "    u = np.random.uniform(0,1)\n",
    "    if u <= 1/3:\n",
    "        return 0\n",
    "    elif u <= 2/3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# The Algorithm    \n",
    "def epsilonGreedy(epsilon):\n",
    "    totalRegret = 0\n",
    "    totalReward = 0\n",
    "    for trial in range(trialNumber):\n",
    "        # Initializing the variables\n",
    "        rewards = np.zeros(N)\n",
    "        reward = 0\n",
    "        count = np.zeros(armNumber)\n",
    "        estimatedMeans = np.zeros(armNumber)\n",
    "        # Conducting the experiment\n",
    "        for i in range(N): # i starts from 0, ends at N-1\n",
    "            # Select and pull the arm\n",
    "            u = np.random.uniform(0,1)\n",
    "            if u <= epsilon:\n",
    "                arm = randomChoose()\n",
    "            else:\n",
    "                arm = np.argmax(estimatedMeans)\n",
    "            # Update the information\n",
    "            rewards[i] = bernSample(theta[arm])\n",
    "            reward += rewards[i]\n",
    "            count[arm] += 1\n",
    "            estimatedMeans[arm] += (rewards[i] - estimatedMeans[arm])/count[arm]\n",
    "        totalRegret += 0.7*N - reward\n",
    "        totalReward += reward\n",
    "    meanRegret = totalRegret/trialNumber\n",
    "    meanReward = totalReward/trialNumber\n",
    "    resualt = [meanReward, meanRegret]\n",
    "    return resualt\n",
    "\n",
    "print(\"Epsilon-greedy Algorithm (0 ≤ epsilon ≤ 1):\")\n",
    "\n",
    "output = epsilonGreedy(0.1)\n",
    "print(\"\\nEpsilon = 0.1:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n",
    "\n",
    "output = epsilonGreedy(0.5)\n",
    "print(\"\\nEpsilon = 0.5:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n",
    "\n",
    "output = epsilonGreedy(0.9)\n",
    "print(\"\\nEpsilon = 0.9:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. UCB (Upper Confidence Bound) Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCB (Upper Confidence Bound) Algorithm:\n",
      "\n",
      "c = 1:\n",
      "Mean aggregated rewards: 3408.260000\n",
      "Mean regret: 91.740000\n",
      "\n",
      "c = 5:\n",
      "Mean aggregated rewards: 2982.380000\n",
      "Mean regret: 517.620000\n",
      "\n",
      "c = 10:\n",
      "Mean aggregated rewards: 2825.430000\n",
      "Mean regret: 674.570000\n"
     ]
    }
   ],
   "source": [
    "# UCB (Upper Confidence Bound) Algorithm\n",
    "def ucb(c):\n",
    "    totalRegret = 0\n",
    "    totalReward = 0\n",
    "    for trial in range(trialNumber):\n",
    "        # Initializing the variables\n",
    "        rewards = np.zeros(N)\n",
    "        reward = 0\n",
    "        count = np.zeros(armNumber)\n",
    "        estimatedMeans = np.zeros(armNumber)\n",
    "        # Conducting the experiment\n",
    "        for i in range(N): # i starts from 0, ends at N-1\n",
    "            if i < armNumber:\n",
    "                rewards[i] = bernSample(theta[i])\n",
    "                reward += rewards[i]\n",
    "                count[i] += 1\n",
    "                estimatedMeans[i] = rewards[i]\n",
    "            else:\n",
    "                # Select and pull the arm\n",
    "                ucb = np.zeros(armNumber)\n",
    "                ucb[0] = estimatedMeans[0] + c * np.sqrt(2*np.log(i)/count[0])    \n",
    "                ucb[1] = estimatedMeans[1] + c * np.sqrt(2*np.log(i)/count[1])\n",
    "                ucb[2] = estimatedMeans[2] + c * np.sqrt(2*np.log(i)/count[2])\n",
    "                arm = np.argmax(ucb)\n",
    "                # Update the information\n",
    "                rewards[i] = bernSample(theta[arm])\n",
    "                reward += rewards[i]\n",
    "                count[arm] += 1\n",
    "                estimatedMeans[arm] += (rewards[i] - estimatedMeans[arm])/count[arm]\n",
    "        totalRegret += 0.7*N - reward\n",
    "        totalReward += reward\n",
    "    meanRegret = totalRegret/trialNumber\n",
    "    meanReward = totalReward/trialNumber\n",
    "    resualt = [meanReward, meanRegret]\n",
    "    return resualt\n",
    "\n",
    "print(\"UCB (Upper Confidence Bound) Algorithm:\")\n",
    "\n",
    "output = ucb(1)\n",
    "print(\"\\nc = 1:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n",
    "\n",
    "output = ucb(5)\n",
    "print(\"\\nc = 5:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n",
    "\n",
    "output = ucb(10)\n",
    "print(\"\\nc = 10:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. TS (Thompson Sampling) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS (Thompson Sampling) Algorithm:\n",
      "\n",
      "Prior parameter: {(1, 1), (1, 1), (1, 1)}:\n",
      "Mean aggregated rewards: 3500.735000\n",
      "Mean regret: -0.735000\n",
      "\n",
      "Prior parameter: {(601, 401), (401, 601), (2, 3)}:\n",
      "Mean aggregated rewards: 3497.585000\n",
      "Mean regret: 2.415000\n"
     ]
    }
   ],
   "source": [
    "# TS (Thompson Sampling) Algorithm\n",
    "def ts(a1, b1, a2, b2, a3, b3):\n",
    "    prior = [[a1, b1], [a2, b2], [a3, b3]]\n",
    "    totalRegret = 0\n",
    "    totalReward = 0\n",
    "    for trial in range(trialNumber):\n",
    "        # Initializing the variables\n",
    "        rewards = np.zeros(N)\n",
    "        reward = 0\n",
    "        estimatedMeans = np.zeros(armNumber)\n",
    "        # Conducting the experiment\n",
    "        for i in range(N): # i starts from 0, ends at N-1\n",
    "            # Sample model\n",
    "            for eachArm in range(armNumber):\n",
    "                estimatedMeans[eachArm] = betaSample(prior[eachArm][0], prior[eachArm][1])\n",
    "            # Select and pull the arm\n",
    "            arm = np.argmax(estimatedMeans)\n",
    "            # Update the Beta distribution\n",
    "            output = bernSample(theta[arm])\n",
    "            prior[arm][0] += output\n",
    "            prior[arm][1] += 1 - output\n",
    "            # Update the information\n",
    "            rewards[i] = output\n",
    "            reward += rewards[i]\n",
    "        totalRegret += 0.7*N - reward\n",
    "        totalReward += reward\n",
    "    meanRegret = totalRegret/trialNumber\n",
    "    meanReward = totalReward/trialNumber\n",
    "    resualt = [meanReward, meanRegret]\n",
    "    return resualt\n",
    "\n",
    "print(\"TS (Thompson Sampling) Algorithm:\")\n",
    "\n",
    "output = ts(1, 1, 1, 1, 1, 1)\n",
    "print(\"\\nPrior parameter: {(1, 1), (1, 1), (1, 1)}:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n",
    "\n",
    "output = ts(601, 401, 401, 601, 2, 3)\n",
    "print(\"\\nPrior parameter: {(601, 401), (401, 601), (2, 3)}:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison and analysis:\n",
    "1. The results of the three algorithms:    \n",
    "\n",
    "| Algorithms                       | $\\epsilon$-greedy           | UCB                   | TS                                           |\n",
    "| -------------------------------- | --------------------------- | --------------------- | -------------------------------------------- |\n",
    "| Runtime(approximation, per test) | 3.6s                        | 6.6s                  | 3.9s                                         | \n",
    "| Mean Regret                      | $\\epsilon=0.1$: 86.415000   | $c = 1$: 91.740000    | {(1, 1), (1, 1), (1, 1)}: -0.735000        |\n",
    "| Mean Regret                      | $\\epsilon=0.5$: 421.590000  | $c = 5$: 517.620000   | {(601, 401), (401, 601), (2, 3)}: 2.415000 |\n",
    "| Mean Regret                      | $\\epsilon=0.9$: 752.195000  | $c = 10$: 674.570000  |    /                                          |\n",
    "\n",
    "   - The results derived by the $\\epsilon$-greedy Algorithm and UCB Algorithm by the given parameters are very close. Going deeper, we may conclude that $\\epsilon$-greedy Algorithm is slightly better (with less regret) with small $\\epsilon (\\text{e.g. } \\epsilon \\leq 0.5)$, and the UCB Algorithm might perform better with larger constant $c (\\text{e.g. } c \\geq 10)$.\n",
    "     \n",
    "   - As for the TS Algorithm, it's obviously better (with much less regret) than the other two algorithms.  \n",
    "   \n",
    "   - The $\\epsilon$-greedy Algorithm and TS Algorithm have close runtime, and they are both faster than $\\epsilon$-greedy Algorithm. (The runtime here and in the table above may vary from different machines, and it's calculated within a single test (consider only one parameter).)\n",
    "   \n",
    "   More detailed analysis for the algorithm **resides as follows**, together with the analysis of their parameters.\n",
    "\n",
    "2. The impacts of their parameters and deeper analysis:  \n",
    "   \n",
    "   **NOTICE: For the cohesion of this essay, the curve of the relation between the regret and different choices of parameters resides in the next part - \"Understanding of the exploration-exploitation trade-off\". Here we just elucidate our idea with natural language.**  \n",
    "\n",
    "   - $\\epsilon$: The regret increases as $\\epsilon$ increases and it's almost linear relation (worst among the three algorithm). Since the randomly generated number $u \\leq \\epsilon$ determines that we do random choice (exploration phase) rather than conducting exploitation, we may derive the conclusion that relatively more exploitation work leads to less regret.  \n",
    "\n",
    "      The reason that this algorithm has a linear worst-case regret is that since it's equally possible to choose each arm, when choosing the third arm (the arm with lowest probability to give reward) every time as the worst case, the regret increments by a constant 0.3 each round. \n",
    "   \n",
    "   - $c$: The regret increases as $c$ increases with sublinear relation (better than the $\\epsilon$-greedy Algorithm). We can derive the conclusion that: when quantitatively delimiting the confidence interval (by multiplying with a constant $c$), the smaller the constant is, the less regret it may cause. Also the constant cannot be too small, or the update is trivial and we may conversely have higher chance to choose those suboptimal arms.   \n",
    "   \n",
    "      This algorithm is also intuitively reasonable, since every time we update the information of an arm, the corresponding confidence interval swells, with a higher confidence level. Therefore, the case that the upper bound of one interval is greater than the other is actually more credible and strict, which accounts for the reasonableness of this algorithm.\n",
    "     \n",
    "      Let's put more consideration on the confidence interval delimitation, both quantitatively and qualitatively:\n",
    "      We cite the conclusion from Bandit Algorithms (T. Lattimore & C. Szepesvari, 2020. Page 117):   \n",
    "      \n",
    "      *For any 1-subgaussian bandit with Algorithm:* $A_t = \\text{arg max}_{i}(\\widehat{\\mu_i}(t+1) + \\sqrt{\\dfrac{2\\log{f(t)}}{T_i(t-1)}})$, where $f(t) = 1 + t\\log^2{(t)}$, *the regret $R_n$ satisfies:*  \n",
    "      \n",
    "      $$\n",
    "      R_n \\leq  \\sum_{i:\\Delta_i > 0}  \\inf_{\\varepsilon\\in(0, \\Delta_i)}\\Delta_i(1+\\dfrac{5}{\\varepsilon^2} + \\dfrac{2(\\log{f(n) + \\sqrt{\\pi\\log{f(n)}} + 1})}{{(\\Delta_i - \\varepsilon)}^2})\n",
    "      $$\n",
    "      \n",
    "      where $\\Delta_i$ is the difference between the max expectation and the real reward in each round.    \n",
    "\n",
    "      *\"This algorithm leads the regret to a constant and such that governs the asymptotic rate of growth of regret.\"* This asymptotic bound is merely achieved by modifying the delimitation of the confidence interval (instead of letting $f(t) = t$ in the version of UCB provided for us), and *\"it's unimprovable in a strong sense.\"*   \n",
    "\n",
    "      This version is better than the origin version: Quantitatively, the regret is lower. Qualitatively, since it leads the regret to a constant (the asymptotic bound), it has better performance especially when time slot is large.\n",
    "   \n",
    "   - $\\alpha_j, \\beta_j$ (defined for the prior distribution): Our analysis is based on the conjugacy of beta distribution. \n",
    "      - If $\\alpha_j$ and $\\beta_j$ are both small numbers (as shown in the case $\\{(1, 1), (1, 1), (1, 1)\\}$), the later experiments can have a big effect on the distribution. \n",
    "   \n",
    "      - If $\\alpha_j$ or $\\beta_j$ is large numbers (as shown in the case $\\{(601, 401), (401, 601), (2, 3)\\}$), the later experiments would have little effect, especially during the first few experiments. This can be easily explained by the conjugacy of beta distribution: for the posterior distribution $\\text{Beta}(a+k, b+n-k)$, $k$ and $n$ have little effect if $a$ or $b$ is large numbers. \n",
    "   \n",
    "      More analysis for this algorithm: In spite of the little difference in the table, the two cases actually perform similarly after we run this algorithm for several times. Since the first case with $\\{(1, 1), (1, 1), (1, 1)\\}$ actually maximizes the effect of the later experiments, we conjectured that the output of the second case still being close is because the choice of the prior distribution for the large numbers $\\{(601, 401), (401, 601)\\}$ obeys the actual probability (oracle value) relation, which leads to similar result as the first case.  \n",
    "      \n",
    "      Given the analysis above, we could learn that the choice of the prior distribution is extremely crucial. Since we have no idea of the exact distribution for each arm, we had better set the parameters of the prior distribution $\\alpha_j, \\beta_j$ as small as possible, e.g. $(1, 1)$. This is aimed to weaken the awful impact of large parameters, since if the prior distribution of large parameters is disparate from the actual distribution (e.g. $\\text{Beta}(200, 800)$ with actual distribution $\\text{Bern}(0.9)$), the offset is overly hard to correct, and the incipient experiments may cause a great deal of regret. This is shown in the code snippet below with a blast regret of **1000.570000**!  \n",
    "\n",
    "      Furthermore, this algorithm does not provide us with deterministic choice like the first two algorithms, instead, it offers us a distribution for the next movement. This \"random choice\" performs well when the environment is no more stochastic. Specifically, when the environment can conversely \"exploit\" **us** by observing our past choices and minimize our reward accordingly, a randomly generated movement is the only approach to deriving a sublinear worst-case regret (Bandit Algorithms, T. Lattimore & C. Szepesvari, 2020. Page 149), which is widely used in dealing with Adversarial Bandits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prior parameter: {(200, 800), (800, 200), (2, 3)}:\n",
      "Mean aggregated rewards: 2499.430000\n",
      "Mean regret: 1000.570000\n"
     ]
    }
   ],
   "source": [
    "# If the prior distribution has large parameters and is disparate from the actual distribution (oracle value)\n",
    "output = ts(200, 800, 800, 200, 2, 3)\n",
    "print(\"\\nPrior parameter: {(200, 800), (800, 200), (2, 3)}:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding of the exploration-exploitation trade-off:\n",
    "It's significant that we choose a proper algorithm and parameters. The idea that drives us to designing the exploration-exploitation mechanism is to lower down the order of growth of the regret to a **sublinear** growth as much as possible.   \n",
    "\n",
    "1. Obviously, the exploration and exploitation somehow contradict with each other. The first algorithm conducts these two phases separately, while the other two algorithm did not. We may draw a vague conclusion that it's not proper to conduct them separately, unless stipulating some rules to optimize the random choosing (exploration) process in $\\epsilon$-greedy, for example, designating weight to the random choice with regard to the result of the prior exploitation phase. Furthermore, since $\\epsilon$ represents the trade-off, by experiment and optimization work we may derive a relatively optimized $\\epsilon$. The implementation of these two ideas are shown as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The regret of Epsilon-Greedy Algorithm for different epsilon\n",
    "\n",
    "Epsilon = np.linspace(0, 1, 100)\n",
    "Regret = [epsilonGreedy(x)[1] for x in Epsilon]\n",
    "plt.plot(Epsilon, Regret)\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the result of the simulation about how the regret is related to $\\epsilon $.\n",
    "\n",
    "Our intuitive conclusions from the figure (suppose $R$ is the regret):\n",
    "\n",
    "- The increase in $\\epsilon$ leads an increase in $R$.\n",
    "- $R=k\\epsilon $, where $k$ is a positive constant. (We can infer that $k\\approx800$ from the figure.)\n",
    "- We may choose $\\epsilon=0$ to optimize this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy Algorithm (0 ≤ epsilon ≤ 1):\n",
      "\n",
      "Epsilon = 0.1:\n",
      "Mean aggregated rewards: 3440.720000\n",
      "Mean regret: 59.280000\n",
      "\n",
      "Epsilon = 0.5:\n",
      "Mean aggregated rewards: 3261.330000\n",
      "Mean regret: 238.670000\n",
      "\n",
      "Epsilon = 0.9:\n",
      "Mean aggregated rewards: 2839.320000\n",
      "Mean regret: 660.680000\n"
     ]
    }
   ],
   "source": [
    "# Our enhanced version of Epsilon-Greedy Algorithm:\n",
    "\n",
    "# By designating weight to the random choice with regard to the result of the prior exploitation phase.\n",
    "def randomChoose(a, b): # a, b represent the prior estimation.\n",
    "    u = np.random.uniform(0, 1)\n",
    "    if u <= a:\n",
    "        return 0\n",
    "    elif u <= b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# The Algorithm    \n",
    "def epsilonGreedy(epsilon):\n",
    "    totalRegret = 0\n",
    "    totalReward = 0\n",
    "    for trial in range(trialNumber):\n",
    "        # Initializing the variables\n",
    "        rewards = np.zeros(N)\n",
    "        reward = 0\n",
    "        count = np.zeros(armNumber)\n",
    "        estimatedMeans = np.zeros(armNumber)\n",
    "        # Conducting the experiment\n",
    "        for i in range(N): # i starts from 0, ends at N-1\n",
    "            # Select and pull the arm by the enhanced version of Epsilon-Greedy Algorithm, learning from the past results.\n",
    "            if i == 0:\n",
    "                a = 1/3\n",
    "                b = 2/3\n",
    "            else:\n",
    "                a = count[0]/i\n",
    "                b = (count[0] + count[1])/i\n",
    "            u = np.random.uniform(0, 1)\n",
    "            if u <= epsilon:\n",
    "                arm = randomChoose(a, b) # Instead of randomChoose()\n",
    "            else:\n",
    "                arm = np.argmax(estimatedMeans)\n",
    "            # Update the information\n",
    "            rewards[i] = bernSample(theta[arm])\n",
    "            reward += rewards[i]\n",
    "            count[arm] += 1\n",
    "            estimatedMeans[arm] += (rewards[i] - estimatedMeans[arm])/count[arm]\n",
    "            # Exploit the player's action and lower the mean if the arm with highest mean was chosen too frequently\n",
    "        totalRegret += 0.7*N - reward\n",
    "        totalReward += reward\n",
    "    meanRegret = totalRegret/trialNumber\n",
    "    meanReward = totalReward/trialNumber\n",
    "    resualt = [meanReward, meanRegret]\n",
    "    return resualt\n",
    "\n",
    "print(\"Epsilon-greedy Algorithm (0 ≤ epsilon ≤ 1):\")\n",
    "\n",
    "output = epsilonGreedy(0.1)\n",
    "print(\"\\nEpsilon = 0.1:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n",
    "\n",
    "output = epsilonGreedy(0.5)\n",
    "print(\"\\nEpsilon = 0.5:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n",
    "\n",
    "output = epsilonGreedy(0.9)\n",
    "print(\"\\nEpsilon = 0.9:\")\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also turns out that this optimization is rather successful especially when choosing a proper $\\epsilon$ (a relatively small one):   \n",
    "The table of regret:\n",
    "| Algorithms/$\\epsilon$ | Original $\\epsilon$-greedy | Our enhanced $\\epsilon$-greedy |\n",
    "| --------------------- | -------------------------- | ------------------------------ |\n",
    "| $\\epsilon=0.1$        | 86.415000                  | 59.280000                      | \n",
    "| $\\epsilon=0.5$        | 421.590000                 | 238.670000                     |\n",
    "| $\\epsilon=0.9$        | 752.195000                 | 660.680000                     |  \n",
    "\n",
    "We conjectured that this enhancement can prevent the linear regret caused by absolute randomness with each probability of $\\frac{1}{3}$, by improving the average-case regret, since we assigned the weight from our past results empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The UCB Algorithm conducts these two phases together in each round:  \n",
    "   \n",
    "   In our implementation, `estimatedMeans[i]` represents the exploitation work, and `c * np.sqrt(2*np.log(i)/count[i])` represents the exploration work with a constant $c$ representing the trade-off. Since we have already talked about the delimitation of the confidence interval in the last part and proposed a optimised version there, we only consider $c$, the weight while balancing two phases, namely trade-off. \n",
    "\n",
    "   From the figure below, we can draw the conclusion that:  \n",
    "\n",
    "   We should put more emphasis on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $R_x$ is the regret when $c=x$. \n",
    "\n",
    "The data we collected tells us that $R_1<R_5<R_{10} $. We can give a intuitive guess that $c=c* $ which optimize this algorithm satisfies that $0<c*<5$. Thus we may find $c*$ by searching the interval $(0,5) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The regret of UCB Algorithm for different c\n",
    "\n",
    "c = np.linspace(0, 5, 100)\n",
    "Regret = [ucb(x)[1] for x in c]\n",
    "plt.plot(c, Regret)\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the result of our simulation about how the regret is related to $c$. \n",
    "\n",
    "Our intuitive conclusions from the figure:\n",
    "\n",
    "- When $c<c*$, the increase in $c$ leads a decrease in the regret.\n",
    "- When $c>c*$, the increase in $c$ leads an increase in the regret.\n",
    "- $0<c*<0.5$.\n",
    "- We may choose $c=c*$ to optimize this algorithm.\n",
    "\n",
    "More accurately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_star = 0.300000\n"
     ]
    }
   ],
   "source": [
    "c_star=np.argmin(Regret) * (5 / 100)\n",
    "print(\"c_star = %f\"%(c_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the simulation tells us that $c*\\approx 0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The TS Algorithm also conducts these two phases together in each round:  \n",
    "\n",
    "    Since this algorithm is based on Bayes Analysis and the conjugacy of Beta distribution, according to our analysis before, we could conclude that:   \n",
    "\n",
    "    This algorithm puts more emphasis on exploration in the initial period (when parameters of Beta distribution is relatively small, indicating lacking of \"experience\"). While during the later period, it's focus veers to the exploitation, with more dependency on the past experience. It's worth nothing that this trade-off is automatically done due to the conjugacy of Beta distribution. \n",
    "    \n",
    "    We surmise that the advantage of this trade-off approach is that more exploration in the initial period ensures the right direction of the later experiments with less regret, and our experiment shows that this is the best approach to trade-off among all the algorithms provided.\n",
    "\n",
    "The exploitation and exploration phases are not absolutely separate. Taken the TS Algorithm for an example, both the initial period and the later period involve exploitation and exploration, and the difference is that for every round which phase we put more emphasis on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the dependent case:\n",
    "1. Our definition of \"dependent\":\n",
    "   \n",
    "   For the independent case, we consider that the distribution of reward of each arm does not change and is independent with each other and with the past choices of the players. With regard to \"dependent\", it refers that the bandit can also somehow learn from the past situations and \"exploit\" the actions of the players in order to minimize our reward. This is related to the problem known as \"Adversarial Bandits\". \n",
    "\n",
    "2. Comparison with the classic Stochastic Bandits and analysis:\n",
    "\n",
    "   NOTE: This part is inspired by: Bandit Algorithms, T. Lattimore & C. Szepesvari, 2020, Chapter 11: The Exp3 Algorithm, and we will quote the idea of the book in ***inclined letter***.\n",
    "\n",
    "   Although the bandit can observe our past actions, we still have two advantages:\n",
    "   \n",
    "   - We always go one step ahead of the bandit.\n",
    "   - Consequently, we can conduct \"exploration\" while the bandit cannot.\n",
    "   \n",
    "   These two advantages ensures that in the process of confronting the evil bandit, we can still optimize our reward.\n",
    "\n",
    "   Besides, the book provides two analysis for the actual algorithms. The relatively weaker one is that *the worst-case stochastic regret is upper-bounded by the worst-case adversarial regret.* And the stronger one says *sublinear worst-case regret is only possible by using a randomised policy.*\n",
    "\n",
    "   This tells us that the core of our algorithm is designing a function that can read our past experiment and generate a new **distribution** of the action in the next round. Note that the algorithm for the Stochastic Bandits that generates deterministic choice is no more functional.\n",
    "\n",
    "3. Design thought:\n",
    "   \n",
    "   The target distribution of our algorithm can actually be regarded as a conditional probability:\n",
    "\n",
    "   $$\n",
    "   P_t = P(A_t = i \\mid A_1, A_2, \\cdots, A_{t-1})(0 < t \\leq N, i\\in\\{1, 2, 3\\})\n",
    "   $$\n",
    "   \n",
    "   where $t$ is the current round and $A_t\\in\\{1, 2, 3\\}$ is the chosen arm.\n",
    "\n",
    "   It's natural that when considering the past experiment, we assign weight to each arm according to our estimated distribution. However, we should also consider: **If the past experiment is reliable (with more round involved or other judgement), we should give more consideration of the weight of the arm with the highest mean, and the distribution curve will gather around that arm. Conversely, if the past experiment is less reliable, less weight will be considered into the arm with the highest mean (still more than those with lower mean), and the distribution curve will be more stable (uniform).** \n",
    "   \n",
    "   The illustration of this intuition is shown below:\n",
    "   \n",
    "   **NOTE: The data presented in the following figures are assumed and used for illustrative purpose to visualize the relationships between data points. They are not real data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAGiCAYAAABOCgSdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAASyUlEQVR4nO3db2xVd/3A8c8tbO2sa6UY1ja2QGSLYSij22CoD1ycg6kl/sHFmGVTn+2BMSFLlJhZmjgXo9NFIfgnZkvcE58wMraEiDgjTrM6OxYJgbikbohliA1dYbRge3xA2p8I7Ndb+untLa9XchPu6Tn3fi7ku71zz72npaIoigAASFJT6QEAgLlNbAAAqcQGAJBKbAAAqcQGAJBKbAAAqcQGAJBKbAAAqcQGAJBKbAAAqcqOjd/97nfR2dkZra2tUSqVYufOnQljAQBzRdmxcfr06Vi5cmVs27YtYx4AYI6ZX+4B99xzT9xzzz0ZswAAc1DZsVGukZGRGBkZmbg/NjYWAwMDsXDhwiiVStlPDwBMg6IoYmhoKFpbW6OmprwTI+mx8eijj0Z3d3f20wAAM+DIkSPxnve8p6xjSkVRFFN9wlKpFE8//XR86lOfuuw+//vOxuDgYLS3t8eRI0eioaFhqk8NAMygN998M9ra2uLkyZPR2NhY1rHp72zU1tZGbW3tRdsbGhrEBgBUmal8BMJ1NgCAVGW/s3Hq1Kl49dVXJ+739fXF/v37o6mpKdrb26d1OACg+pUdGy+99FLceeedE/c3bdoUEREPPPBAPPnkk9M2GAAwN5QdGx/5yEfiCj5TCgBcZXxmAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBIJTYAgFRiAwBINb/SA0yH0bEievoG4vjQcCy6vi5WL22KeTWlSo8FAMQciI3dB/qje9fB6B8cntjW0lgXXZ3LY/2KlgpOBgBEVPlplN0H+uPBp3ovCI2IiGODw/HgU72x+0B/hSYDAMZVbWyMjhXRvetgFJf42fi27l0HY3TsUnsAADOlamOjp2/gonc0/lsREf2Dw9HTNzBzQwEAF6na2Dg+dPnQmMp+AECOqo2NRdfXTet+AECOqo2N1UuboqWxLi73BddSnP9WyuqlTTM5FgDwP6o2NubVlKKrc3lExEXBMX6/q3O5620AQIVVbWxERKxf0RLb7+uI5sYLT5U0N9bF9vs6XGcDAGaBqr+o1/oVLfGx5c2uIAoAs1TVx0bE+VMqa9+7sNJjAACXUNWnUQCA2U9sAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkEpsAACpxAYAkGp+pQcA5obRsSJ6+gbi+NBwLLq+LlYvbYp5NaVKjwXMAmIDuGK7D/RH966D0T84PLGtpbEuujqXx/oVLRWcDJgNnEYBrsjuA/3x4FO9F4RGRMSxweF48Kne2H2gv0KTAbOF2ACmbHSsiO5dB6O4xM/Gt3XvOhijY5faA7haiA1gynr6Bi56R+O/FRHRPzgcPX0DMzcUMOuIDWDKjg9dPjSmsh8wN4kNYMoWXV83rfsBc5PYAKZs9dKmaGmsi8t9wbUU57+Vsnpp00yOBcwyYgOYsnk1pejqXB4RcVFwjN/v6lzuehtwlRMbwBVZv6Iltt/XEc2NF54qaW6si+33dbjOBuCiXsCVW7+iJT62vNkVRIFLEhvAtJhXU4q1711Y6TGAWchpFAAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFKJDQAgldgAAFLNr/QAAMCVGx0roqdvII4PDcei6+ti9dKmmFdTqvRYESE2AKDq7T7QH927Dkb/4PDEtpbGuujqXB7rV7RUcLLznEYBgCq2+0B/PPhU7wWhERFxbHA4HnyqN3Yf6K/QZP9HbABAlRodK6J718EoLvGz8W3duw7G6Nil9pg5YgMAqlRP38BF72j8tyIi+geHo6dvYOaGugSxAQBV6vjQ5UNjKvtlERsAUKUWXV83rftlERsAUKVWL22Klsa6uNwXXEtx/lspq5c2zeRYFxEbAFCl5tWUoqtzeUTERcExfr+rc3nFr7chNgCgiq1f0RLb7+uI5sYLT5U0N9bF9vs6ZsV1NlzUCwCq3PoVLfGx5c2uIAoA5JlXU4q1711Y6TEuyWkUACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACCV2AAAUokNACDVlGJj27ZtsWTJkqirq4s1a9ZET0/PdM8FAMwRZcfGL3/5y9i0aVN0dXVFb29vrFy5MtatWxfHjx/PmA8AqHKloiiKcg5Ys2ZN3H777bF169aIiBgbG4u2trb4yle+El//+tcv2n9kZCRGRkYm7g8ODkZ7e3scOXIkGhoarnB8AGAmvPnmm9HW1hYnT56MxsbGso6dX87OZ8+ejT//+c+xefPmiW01NTVx1113xR//+MdLHvPoo49Gd3f3Rdvb2trKGhQAqLyhoaHc2Dhx4kSMjo7GDTfccMH2G264IQ4dOnTJYzZv3hybNm2auD82NhYDAwOxcOHCKJVKZQ37dsaLyzsmUDnWIVRW5hosiiKGhoaitbW17GPLio2pqK2tjdra2gu2vetd70p7voaGBv+RgwqzDqGystZgue9ojCvrA6Lvfve7Y968efHGG29csP2NN96I5ubmKQ0AAMxtZcXGtddeG7feemvs3bt3YtvY2Fjs3bs31q5dO+3DAQDVr+zTKJs2bYoHHnggbrvttli9enU8/vjjcfr06fjSl76UMd+k1dbWRldX10WnbICZYx1CZc3WNVj2V18jIrZu3Rrf/e5349ixY3HLLbfED3/4w1izZk3GfABAlZtSbAAATJbfjQIApBIbAEAqsQEApKp4bPz2t7+NUqkUJ0+erPQocNWyDqGy5voaTI2NUqn0trctW7ZM63Pt3Llz2h4vImLHjh1x9913T1xaff/+/dP6+DATqnkdnjt3Lr72ta/F+9///qivr4/W1ta4//774x//+Me0PQdkq+Y1GBGxZcuWeN/73hf19fWxYMGCuOuuu+LFF18s6zFSY6O/v3/i9vjjj0dDQ8MF2x566KHMp79ip0+fjg9/+MPxne98p9KjwJRV8zp86623ore3Nx5++OHo7e2NHTt2xOHDh2PDhg2VHg0mrZrXYETETTfdFFu3bo2//OUv8fvf/z6WLFkSd999d/zzn/+c/IMUM+SJJ54oGhsbL9r+/PPPFxFR/PrXvy5uvfXW4rrrrivWrl1bHDp06IL9du7cWaxataqora0tli5dWmzZsqU4d+5cURRFsXjx4iIiJm6LFy8uiqIoXn311WLDhg3FokWLivr6+uK2224r9uzZU/bsfX19RUQUL7/8ctnHwmxSzetwXE9PTxERxWuvvTblx4BKmQtrcHBwcGLWyar4ZzbGfeMb34jHHnssXnrppZg/f358+ctfnvjZvn374v7774+vfvWrcfDgwfjJT34STz75ZDzyyCMREfGnP/0pIiKeeOKJ6O/vn7h/6tSp+PjHPx579+6Nl19+OdavXx+dnZ3x+uuvz/wLhCpQDetwcHAwSqVS6i90hEqZ7Wvw7Nmz8dOf/jQaGxtj5cqVkz9wymlTpsnU3LjnnnuuiIjizJkzRVEUxUc/+tHi29/+9gXH/eIXvyhaWlom7kdE8fTTT/+/c9x8883Fj370o7Jm984Gc0U1r8OiKIozZ84UHR0dxRe+8IWyj4XZoFrX4K5du4r6+vqiVCoVra2tRU9Pz6SPLYqiSP8V85P1gQ98YOLPLS0tERFx/PjxaG9vj1deeSVeeOGFiXqLiBgdHY3h4eF466234h3veMclH/PUqVOxZcuWeO6556K/vz/+/e9/x5kzZ7yzAZcxm9fhuXPn4t57742iKGL79u1TeHUw+83WNXjnnXfG/v3748SJE/Gzn/0s7r333njxxRdj0aJFkzp+1sTGNddcM/HnUqkUEed/o2zE+b+o7u7u+MxnPnPRcXV1dZd9zIceeij27NkT3/ve92LZsmVx3XXXxcaNG+Ps2bPTPD3MDbN1HY6HxmuvvRa/+c1voqGhYdLHQjWZrWuwvr4+li1bFsuWLYs77rgjbrzxxvj5z38emzdvntTxsyY23k5HR0ccPnw4li1bdtl9rrnmmhgdHb1g2wsvvBBf/OIX49Of/nREnP+H+tvf/pY5KsxZlVqH46Hx17/+NZ5//vlYuHDhlOaHajeb/l84NjYWIyMjk96/KmLjm9/8Znzyk5+M9vb22LhxY9TU1MQrr7wSBw4ciG9961sREbFkyZLYu3dvfOhDH4ra2tpYsGBB3HjjjbFjx47o7OyMUqkUDz/88EQhTsbAwEC8/vrrE9/pP3z4cERENDc3R3Nz8/S/UJjFKrEOz507Fxs3boze3t549tlnY3R0NI4dOxYREU1NTXHttdemvV6YbSqxBk+fPh2PPPJIbNiwIVpaWuLEiROxbdu2OHr0aHzuc5+b9Oyz5tsob2fdunXx7LPPxq9+9au4/fbb44477ogf/OAHsXjx4ol9HnvssdizZ0+0tbXFqlWrIiLi+9//fixYsCA++MEPRmdnZ6xbty46Ojom/bzPPPNMrFq1Kj7xiU9ERMTnP//5WLVqVfz4xz+e3hcIVaAS6/Do0aPxzDPPxN///ve45ZZboqWlZeL2hz/8IeV1wmxViTU4b968OHToUHz2s5+Nm266KTo7O+Nf//pX7Nu3L26++eZJz+5XzAMAqarinQ0AoHpdtbGxb9++eOc733nZG5DPOoTKmqk1eNWeRjlz5kwcPXr0sj9/u0/7AtPDOoTKmqk1eNXGBgAwM67a0ygAwMwQGwBAKrEBAKQSGwBAKrEBAKQSGwBAKrEBAKT6D7MvirgYCtpKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The distribution curve gathers around the arm with the highest mean, this involves more exploitation.\n",
    "x = [1/3, 2/3, 1]\n",
    "y = [0.8, 0.3, 0.1] #assumed\n",
    "plt.scatter(x, y)\n",
    "x_labels = ['Theta_{}'.format(i+1) for i in range(len(x))]\n",
    "plt.xticks(x, x_labels)\n",
    "plt.yticks([0, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The distribution curve is more stable (uniform) around the arm with the highest mean, this involves more exploration.\n",
    "\n",
    "y = [0.8, 0.7, 0.6] #assumed\n",
    "plt.scatter(x, y)\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(x, x_labels)\n",
    "plt.yticks([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   This is actually a kind of exploration-exploitation trade-off: when the distribution curve gathers around an arm, we are actually putting more emphasis on exploitation, and **this is done at the initial period**. If the distribution curve is more stable (uniform), we are doing more exploration, and **this is done at later period**.\n",
    "\n",
    "   Fortunately, we have found a very suitable algorithm for this, which is **Softmax Transformation**:\n",
    "\n",
    "   This is a method for converting a set of real numbers (in this problem: weight - $\\{w_1, w_2, w_3\\}$) into a probability distribution. \n",
    "   \n",
    "\n",
    "   $$\n",
    "   p_i = \\dfrac{e^{w_i/\\tau}}{\\displaystyle\\sum_{j}e^{w_j/\\tau}}, (i \\in \\{1,2,3\\}, \\tau > 0)\n",
    "   $$\n",
    "   \n",
    "   \n",
    "   It has a parameter $\\tau$ (a.k.a. the temperature parameter) that determines how much consideration of wight we make when designing the new distribution. A greater $\\tau$ leads to a more stable (uniform) distribution, while a smaller $\\tau$ leads to more concentration of probabilities around the arms with higher weights (means). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Design details:\n",
    "   \n",
    "   - How to design a bandit that can learn and exploit: since our algorithm should have sublinear regret with respect to time, we are actually putting more emphasis on the arm with the highest true mean. Therefore, the bandit will observe that if we choose the arm with the highest mean too frequently (this is defined by a proportionality coefficient according to the proportion of their mean), it will automatically modulate the award ditribution of that arm and lower it's mean (but the order of the three means still remains).  \n",
    "   \n",
    "   - The wight is assigned by the mean of the latest estimated distribution for each arm. \n",
    "\n",
    "   - A new choice distribution is generated according to the weight and Softmax Transformation. And we sample from that new distribution to decide our next choice. Note that $\\tau$ is decreasing with more experiments and more certainty of the distribution.\n",
    "\n",
    "The Python code is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean aggregated rewards: 3228.530000\n",
      "Mean regret: 271.470000\n"
     ]
    }
   ],
   "source": [
    "# Algorithm for Adversarial Bandits\n",
    "def randomChoose(a, b): # a, b represent the consideration of weights in the new distribution, 0 < a < b < 1.\n",
    "    u = np.random.uniform(0,1)\n",
    "    if u <= a:\n",
    "        return 0\n",
    "    elif u <= b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# The Algorithm    \n",
    "def adverseBandit(c): # c is a constant used for defining the temperature parameter tau\n",
    "    totalRegret = 0\n",
    "    totalReward = 0\n",
    "    for trial in range(trialNumber):\n",
    "        # Initializing the variables\n",
    "        rewards = np.zeros(N)\n",
    "        reward = 0\n",
    "        count = np.zeros(armNumber)\n",
    "        estimatedMeans = np.zeros(armNumber)\n",
    "        # Conducting the experiment\n",
    "        for i in range(N): # i starts from 0, ends at N-1\n",
    "            tau = c / (i+1) # The temperature parameter, we will expand on this below.\n",
    "            # Select and pull the arm by the enhanced version of Epsilon-Greedy Algorithm, learning from the past results.\n",
    "            sum = estimatedMeans[0] + estimatedMeans[1] + estimatedMeans[2]\n",
    "            if sum == 0:\n",
    "                w0 = 1\n",
    "                w1 = 1\n",
    "                w2 = 1\n",
    "            else:\n",
    "                # The wight is assigned by the mean of the latest estimated ditribution for each arm:\n",
    "                w0 = estimatedMeans[0]/sum\n",
    "                w1 = estimatedMeans[1]/sum\n",
    "                w2 = estimatedMeans[2]/sum\n",
    "            sum = math.exp(w0/tau) + math.exp(w1/tau) + math.exp(w2/tau)\n",
    "            # Implementing the Softmax Transformation\n",
    "            a = math.exp(w0/tau)/sum\n",
    "            b = math.exp(w0/tau)/sum + math.exp(w1/tau)/sum\n",
    "            # The arm sampled from the new distribution:\n",
    "            arm = randomChoose(a, b) \n",
    "            # Update the information:\n",
    "            rewards[i] = bernSample(theta[arm])\n",
    "            reward += rewards[i]\n",
    "            count[arm] += 1\n",
    "            estimatedMeans[arm] += (rewards[i] - estimatedMeans[arm])/count[arm]\n",
    "            # Exploit the player's action and lower the mean if the arm with highest mean was chosen too frequently\n",
    "            if count[0] > 0.7 * i: # If the arm 0 was chosen too frequently\n",
    "                theta[0] -= 0.1 / (i+1) * (theta[0] - theta[1] - 0.15) # Lower the mean of arm 0\n",
    "        totalRegret += 0.7*N - reward\n",
    "        totalReward += reward\n",
    "    meanRegret = totalRegret/trialNumber\n",
    "    meanReward = totalReward/trialNumber\n",
    "    resualt = [meanReward, meanRegret]\n",
    "    return resualt\n",
    "\n",
    "output = adverseBandit(10)\n",
    "print(\"Mean aggregated rewards: %f\" %(output[0]))\n",
    "print(\"Mean regret: %f\" %(output[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the code: \n",
    "\n",
    "1. The update rule of the estimated means is the same as the UCB.\n",
    "\n",
    "2. We define the $\\tau$ by `tau = c / (i+1)`: the numerator $c$ can be numbers from 9 to 23, and this is interval chosen deliberately after testing integers from 8 to 100, and the denominator is chosen to be linear with $i$ after testing sublinear functions like log and square root. Also, this cannot exceed linear too much since if the time slot is large, there might be `math range error`.\n",
    "\n",
    "3. The bandit lowers the probability by `theta[0] -= 0.1 / (i+1) * (theta[0] - theta[1] - 0.15)`. This is mostly designed by avoiding the maximum oracle mean `theta[0]` to decrease under or too close to the second maximum `theta[1]`.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "This algorithm performs well when the bandit can learn and exploit by itself. And if we choose not to generate any distribution, but only choose the arm with the greatest estimated mean (generate a deterministic), the regret is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Bayesian Bandit Algorithms(Optional)\n",
    "### Check the behavior of the policy\n",
    "Given the prior parameters $\\alpha_i , \\beta_i$, success probability $\\theta_i$ and the reward $\\gamma $, now we are computing the regret using the policy that choose the arm with maximal expected $\\theta_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation code\n",
    "def check(alpha1, beta1, alpha2, beta2, theta_1, theta_2, Gamma):\n",
    "    totalRegret = 0\n",
    "    for i in range(trialNumber):\n",
    "        reward = 0 # total reward of each trail\n",
    "        oracle_value = 0\n",
    "        reward_j = 1 # the reward of the j-th pull\n",
    "        maxTheta = max(theta_1, theta_2)\n",
    "        for j in range(N):\n",
    "            oracle_value += maxTheta * reward_j\n",
    "            expected_theta_1 = alpha1 / (alpha1 + beta1) \n",
    "            expected_theta_2 = alpha2 / (alpha2 + beta2)\n",
    "            u = random.uniform(0, 1)\n",
    "            if expected_theta_1 >= expected_theta_2 : # choose the first arm\n",
    "               if u <= theta_1 :\n",
    "                   reward += reward_j\n",
    "                   alpha1 += 1\n",
    "               else:\n",
    "                   beta1 += 1\n",
    "            else:   # choose the second arm\n",
    "                if u <= theta_2 :\n",
    "                   reward += reward_j\n",
    "                   alpha2 += 1\n",
    "                else:\n",
    "                   beta2 += 1\n",
    "            reward_j *= Gamma\n",
    "        totalRegret += oracle_value - reward\n",
    "    return totalRegret/trialNumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\theta_1 =0.7, \\theta_2 =0.2,\\gamma=0.7$. Now we are checking the behaviour of this policy given $(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)=(1,1,1,1) $, $(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)=(2,1,1,2)$ and $(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)=(1,2,2,1)$.\n",
    "\n",
    "For the convenience of implementation, **we set the time slot to be $N$(5000) instead of infinity.** This is reasonable since the tail experiments generate trivial regret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle value: 3.333333\n",
      "\n",
      "Prior parameter: [1, 1, 1, 1]\n",
      "Mean reward: 3.388842\n",
      "Mean regret: -0.055508\n",
      "\n",
      "Prior parameter: [2, 1, 1, 2]\n",
      "Mean reward: 3.335177\n",
      "Mean regret: -0.001844\n",
      "\n",
      "Prior parameter: [1, 2, 2, 1]\n",
      "Mean reward: 3.284302\n",
      "Mean regret: 0.049031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameter = [[1, 1, 1, 1], [2, 1, 1, 2], [1, 2, 2, 1]]\n",
    "theta_1 = 0.7\n",
    "theta_2 = 0.2\n",
    "Gamma = 0.7\n",
    "oracleValue = 0\n",
    "for i in range(N): oracleValue += Gamma ** i\n",
    "print(\"Oracle value: %f\\n\"%(oracleValue))\n",
    "for data in parameter:\n",
    "    regret = check(data[0], data[1], data[2], data[3], theta_1, theta_2, Gamma)\n",
    "    print(\"Prior parameter:\",data)\n",
    "    print(\"Mean reward: %f\" %(oracleValue - regret))\n",
    "    print(\"Mean regret: %f\\n\" %(regret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A counter-example leading to suboptimal regret:\n",
    "\n",
    "As is mentioned in the analysis of TS algorithm in part I, the choice of the prior distribution has enormous effect on the regret, and if we are not absolutely sure about the relationships of the real distribution, we had better set the parameters of the prior distribution $\\alpha_j, \\beta_j$ as small as possible, e.g. $(1, 1)$. (As is analyzed before, this is aimed to weaken the awful impact of large parameters, since if the prior distribution of large parameters is disparate from the actual distribution (oracle value) (e.g. $\\text{Beta}(200, 800)$ with actual distribution $\\text{Bern}(0.9)$), the offset is overly hard to correct, and the incipient experiments may cause a great amount of regret.)\n",
    "\n",
    "The code snippet exemplifying this is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle value: 3.333333\n",
      "\n",
      "Prior parameter: [150, 850, 850, 150]\n",
      "Mean regret: 1.677078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [150, 850, 850, 150]\n",
    "theta_1 = 0.7\n",
    "theta_2 = 0.2\n",
    "Gamma = 0.7\n",
    "oracleValue = 0\n",
    "for i in range(N): oracleValue += Gamma ** i\n",
    "print(\"Oracle value: %f\\n\"%(oracleValue))\n",
    "regret = check(data[0], data[1], data[2], data[3], theta_1, theta_2, Gamma)\n",
    "print(\"Prior parameter:\",data)\n",
    "print(\"Mean regret: %f\\n\" %(regret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we could draw the conclusion that when the prior distribution is disparate from the actual distribution with large parameters, the regret will be high. (Since $\\gamma$ ranges from 0 to 1, the regret seems still trivial, but now it takes up much higher proportion of the oracle value.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The proof of the equation\n",
    "Suppose\n",
    "\n",
    "$$\n",
    "R_1(\\alpha_1,\\beta_1)=\\frac{\\alpha_1}{\\alpha_1+\\beta_1}(1+\\gamma R(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2))+\\frac{\\beta_1}{\\alpha_1+\\beta_1}\\gamma R(\\alpha_1,\\beta_1+1,\\alpha_2,\\beta_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_2(\\alpha_2,\\beta_2)=\\frac{\\alpha_2}{\\alpha_2+\\beta_2}(1+\\gamma R(\\alpha_1,\\beta_1,\\alpha_2+1,\\beta_2))+\\frac{\\beta_2}{\\alpha_2+\\beta_2}\\gamma R(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2+1)\n",
    "$$\n",
    "Then we have $R(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)=\\max\\{R_1(\\alpha_1,\\beta_1),R_2(\\alpha_2,\\beta_2)\\} $, where $R(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)$ is the expected total reward under an optimal policy when the prior parameters are $\\alpha_1,\\beta_1,\\alpha_2,\\beta_2$.\n",
    "\n",
    "#### proof\n",
    "Let $R_1'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)$ be the expected total reward under an optimal policy if we choose arm 1 in the first pull, and we define $R_2'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)$ similarly. Since the first pull is either arm 1 or arm 2, we have $R(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)=\\max\\{R_1'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2),R_2'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)\\} $.\n",
    "\n",
    "Suppose we choose arm 1 in the first pull:\n",
    "\n",
    "If the first pull results in a success, we will obtain 1 as a reward. Then we update the prior parameters to $(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2) $ for the next pull. Now we start a new process and regard the second pull as the first pull, the expected total reward under an optimal policy is $R(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2)$. Since the reward is $\\gamma^{t-1}$ at $t^{th}$ pull and we pull infinite times, the reward at $s^{th}$ pull in the new process is the reward at $s+1^{st} $ pull in the original process, $\\text{i.e.} \\gamma^s=\\gamma \\cdot \\gamma^{s-1} $. Then we will obtain $\\gamma R(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2) $ in reward. Thus the total reward is $1+\\gamma R(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2) $ if the first pull succeeds.\n",
    " \n",
    "If the first pull fails, we will not obtain any reward. And then we update the prior parameters to $(\\alpha_1,\\beta_1+1,\\alpha_2,\\beta_2) $ for the next pull. Similarly as before, we will obtain $\\gamma R(\\alpha_1,\\beta_1+1,\\alpha_2,\\beta_2)$ as a reward if we follow the optimal policy.\n",
    "\n",
    "Since the first pull succeeds with expected probability $\\dfrac{\\alpha_1}{\\alpha_1+\\beta_1} $ and fails with expected probability $\\dfrac{\\beta_1}{\\alpha_1+\\beta_1}$, the expected total reward we obtain if we pull arm 1 first is \n",
    "\n",
    "\\begin{align*}\n",
    "R_1'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)&=E[E(\\theta_1 (1+\\gamma R(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2))+(1-\\theta_1)\\gamma R(\\alpha_1,\\beta_1+1,\\alpha_2,\\beta_2)\\mid{(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)})]\\\\&\n",
    "=E[\\theta_1 (1+\\gamma R(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2))+(1-\\theta_1)\\gamma R(\\alpha_1,\\beta_1+1,\\alpha_2,\\beta_2)]\\\\&\n",
    "=\\frac{\\alpha_1}{\\alpha_1+\\beta_1}(1+\\gamma R(\\alpha_1+1,\\beta_1,\\alpha_2,\\beta_2))+\\frac{\\beta_1}{\\alpha_1+\\beta_1}\\gamma R(\\alpha_1,\\beta_1+1,\\alpha_2,\\beta_2)\n",
    "\\end{align*}\n",
    "By symmetric,\n",
    "\n",
    "$$\n",
    "R_2'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)=\\frac{\\alpha_2}{\\alpha_2+\\beta_2}(1+\\gamma R(\\alpha_1,\\beta_1,\\alpha_2+1,\\beta_2))+\\frac{\\beta_2}{\\alpha_2+\\beta_2}\\gamma R(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2+1)\n",
    "$$\n",
    "Note that $R_1(\\alpha_1,\\beta_1)=R_1'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2),R_2(\\alpha_2,\\beta_2)=R_2'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)$, we obtain \n",
    "$$\n",
    "R(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)=\\max\\{R_1'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2),R_2'(\\alpha_1,\\beta_1,\\alpha_2,\\beta_2)\\}=\\max\\{R_1(\\alpha_1,\\beta_1),R_2(\\alpha_2,\\beta_2)\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ending\n",
    "Bandit learning is significant for understanding decision-making under uncertainty. It has applications in psychology, behavioral economics, AI, and helps analyze choices, rewards, and optimization strategies in various domains, such as market competition and machine learning. In computer science, bandit learning problem has extensive applications, particularly in reinforcement learning and algorithm optimization.\n",
    "\n",
    "Our study has presented the implementation, simulation, analysis and optimization of some classical algorithms for Stochastic Bandits and explored the Adversarial Bandits by raising a self-designed algorithm. Besides, we also conducted some further analysis with regard to Bayesian Bandits if the reward is decreasing as we pull. We have displayed the behaviour of the policy that choose the arm with maximal expected $\\theta_i$ in such a situation. Then we have also shown that this is not optimal policy given the prior parameters and found the reward in the optimal policy.\n",
    "\n",
    "However, this study is rather \"superficial\" and there are still much more issues that warrant further exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
